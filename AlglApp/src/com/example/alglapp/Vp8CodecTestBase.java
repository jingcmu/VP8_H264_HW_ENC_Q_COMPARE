/*
 * Copyright (C) 2013 The Android Open Source Project
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.example.alglapp;
//package com.example.android.vp8activity;
//package com.google.android.xts.media;

import android.content.Context;
import android.content.res.Resources;
import android.media.MediaCodec;
import android.media.MediaCodecInfo.CodecCapabilities;
import android.media.MediaCodecList;
import android.media.MediaCodecInfo;
import android.media.MediaFormat;
import android.os.Bundle;
import android.os.Environment;
import android.os.Looper;
import android.os.Handler;
import android.os.SystemClock;
import android.test.AndroidTestCase;
import android.util.Log;
import android.util.Pair;
//import com.google.android.xts.media.R;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.InputStream;
import java.nio.ByteBuffer;
import java.util.Locale;
import java.util.ArrayList;
import java.util.concurrent.Callable;
import java.util.concurrent.CountDownLatch;

/**
 * Verification test for vp8 encoder and decoder.
 *
 * A raw yv12 stream is encoded at various settings and written to an IVF
 * file. Encoded stream bitrate and key frame interval are checked against target values.
 * The stream is later decoded by vp8 decoder to verify frames are decodable and to
 * calculate PSNR values for various bitrates.
 */
public class Vp8CodecTestBase extends AndroidTestCase {

    protected static final String TAG = "VP8CodecTestBase";
    //private static final String VP8_MIME = "video/x-vnd.on2.vp8";
    //private static final String VPX_SW_DECODER_NAME = "OMX.google.vp8.decoder";
    //private static final String VPX_SW_ENCODER_NAME = "OMX.google.vp8.encoder";
    private static final String VP8_MIME = "video/avc";
    private static final String VPX_SW_DECODER_NAME = "OMX.google.h264.decoder";
    private static final String VPX_SW_ENCODER_NAME = "OMX.google.h264.encoder";
    private static final String OMX_SW_CODEC_PREFIX = "OMX.google";
    protected static final String SDCARD_DIR =
            Environment.getExternalStorageDirectory().getAbsolutePath();

    // Default timeout for MediaCodec buffer dequeue - 1000 ms.
    protected static final long DEFAULT_TIMEOUT_US = 1000000;
    // Default sync frame interval in frames (zero means allow the encoder to auto-select
    // key frame interval).
    private static final int SYNC_FRAME_INTERVAL = 0;
    // Video bitrate type - should be set to OMX_Video_ControlRateConstant from OMX_Video.h
    protected static final int VIDEO_ControlRateVariable = 1;
    protected static final int VIDEO_ControlRateConstant = 2;
    // NV12 color format supported by QCOM codec, but not declared in MediaCodec -
    // see /hardware/qcom/media/mm-core/inc/OMX_QCOMExtns.h
    private static final int COLOR_QCOM_FORMATYUV420PackedSemiPlanar32m = 0x7FA30C04;
    // Allowable color formats supported by codec - in order of preference.
    private static final int[] mSupportedColorList = {
            CodecCapabilities.COLOR_FormatYUV420Planar,
            CodecCapabilities.COLOR_FormatYUV420SemiPlanar,
            CodecCapabilities.COLOR_QCOM_FormatYUV420SemiPlanar,
            COLOR_QCOM_FORMATYUV420PackedSemiPlanar32m
    };
    // Scaled image cache list - contains scale image dimensions, for which up-scaled frames
    // were calculated and were written to yuv file.
    ArrayList<Pair<Integer, Integer>> mScaledImages = new ArrayList<Pair<Integer, Integer>>();

    private Resources mResources;

    @Override
    public void setContext(Context context) {
        super.setContext(context);
        mResources = mContext.getResources();
    }

    /**
     *  VP8 codec properties generated by getVp8CodecProperties() function.
     */
    protected class CodecProperties {
        CodecProperties(String codecName, int colorFormat) {
            this.codecName = codecName;
            this.colorFormat = colorFormat;
        }
        public boolean  isGoogleSwCodec() {
            return codecName.startsWith(OMX_SW_CODEC_PREFIX);
        }

        public final String codecName; // OpenMax component name for VP8 codec.
        public final int colorFormat;  // Color format supported by codec.
    }

    /**
     * Function to find VP8 codec.
     *
     * Iterates through the list of available codecs and tries to find
     * VP8 codec, which can support either YUV420 planar or NV12 color formats.
     * If forceSwGoogleCodec parameter set to true the function always returns
     * Google sw VP8 codec.
     * If forceSwGoogleCodec parameter set to false the functions looks for platform
     * specific VP8 codec first. If no platform specific codec exist, falls back to
     * Google sw VP8 codec.
     *
     * @param isEncoder     Flag if encoder is requested.
     * @param forceSwGoogleCodec  Forces to use Google sw codec.
     */
    protected CodecProperties getVp8CodecProperties(boolean isEncoder,
            boolean forceSwGoogleCodec) throws Exception {
        CodecProperties codecProperties = null;

        if (!forceSwGoogleCodec) {
            // Loop through the list of omx components in case platform specific codec
            // is requested.
            for (int i = 0; i < MediaCodecList.getCodecCount(); i++) {
                MediaCodecInfo codecInfo = MediaCodecList.getCodecInfoAt(i);
                if (isEncoder != codecInfo.isEncoder()) {
                    continue;
                }
                Log.v(TAG, codecInfo.getName());
                // Check if this is sw Google codec - we should ignore it.
                boolean isGoogleSwCodec = codecInfo.getName().startsWith(OMX_SW_CODEC_PREFIX);
                if (isGoogleSwCodec) {
                    continue;
                }

                for (String type : codecInfo.getSupportedTypes()) {
                    if (!type.equalsIgnoreCase(VP8_MIME)) {
                        continue;
                    }
                    CodecCapabilities capabilities = codecInfo.getCapabilitiesForType(VP8_MIME);

                    // Get candidate codec properties.
                    Log.v(TAG, "Found candidate codec " + codecInfo.getName());
                    for (int colorFormat : capabilities.colorFormats) {
                        Log.v(TAG, "   Color: 0x" + Integer.toHexString(colorFormat));
                    }

                    // Check supported color formats.
                    for (int supportedColorFormat : mSupportedColorList) {
                        for (int codecColorFormat : capabilities.colorFormats) {
                            if (codecColorFormat == supportedColorFormat) {
                                codecProperties = new CodecProperties(codecInfo.getName(),
                                        codecColorFormat);
                                Log.v(TAG, "Found target codec " + codecProperties.codecName +
                                        ". Color: 0x" + Integer.toHexString(codecColorFormat));
                                return codecProperties;
                            }
                        }
                    }
                    // HW codec we found does not support one of necessary color formats.
                    throw new RuntimeException("No hw codec with YUV420 or NV12 color formats");
                }
            }
        }
        // If no hw vp8 codec exist or sw codec is requested use default Google sw codec.
        if (codecProperties == null) {
            Log.v(TAG, "Use SW VP8 codec");
            if (isEncoder) {
                codecProperties = new CodecProperties(VPX_SW_ENCODER_NAME,
                        CodecCapabilities.COLOR_FormatYUV420Planar);
            } else {
                codecProperties = new CodecProperties(VPX_SW_DECODER_NAME,
                        CodecCapabilities.COLOR_FormatYUV420Planar);
            }
        }

        return codecProperties;
    }

    /**
     * Parameters for encoded and decoded video stream.
     */
    protected class CodecStreamParameters {
        // Name of raw YUV420 input file. When the value of this parameter
        // is set to null input file descriptor from inputResourceId parameter
        // is used instead.
        public String inputYuvFilename;
        // Name of scaled YUV420 input file.
        public String scaledYuvFilename;
        // File descriptor for the raw input file (YUV420). Used only if
        // inputYuvFilename parameter is null.
        int inputResourceId;
        // Name of the IVF file to write encoded bitsream
        public String encodedIvfFilename;
        // Name of the YUV420 file to write decoded frames
        public String outputYuvFilename;
        // Force to use Google SW VP8 codec.
        boolean forceSwCodec;
        // Number of frames to encode.
        int frameCount;
        // Encoding frame rate in frames per second for every frame. If array length
        // is shorter than the total number of frames, the last value is re-used for
        // all remaining frames. For constant framerate encoding single element
        // array can be used with first element set to target framerate value.
        int frameRateSet[];
        // Encoded frame width.
        public int frameWidth;
        // Encoded frame height.
        public int frameHeight;
        // Encoding bitrate array in bits/second for every frame. If array length
        // is shorter than the total number of frames, the last value is re-used for
        // all remaining frames. For constant bitrate encoding single element
        // array can be used with first element set to target bitrate value.
        public int[] bitrateSet;
        // Encoding bitrate type - VBR or CBR
        public int bitrateType;
        // Number of temporal layers
        public int temporalLayers;
        // Desired key frame interval - codec is asked to generate key frames
        // at a period defined by this parameter.
        public int syncFrameInterval;
        // Optional parameter - forced key frame interval. Used to
        // explicitly request the codec to generate key frames using
        // MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME parameter.
        public int syncForceFrameInterval;
        // Buffer timeout
        long timeoutDequeue;
        // Flag if encoder should run in Looper thread.
        boolean runInLooperThread;
        // Decoding rate decimator - used to test base temporal layer decoding
        int decodingRateDecimator;
    }

    /**
     * Generates an array of default parameters for encoder output stream based on
     * upscaling value.
     */
    protected ArrayList<CodecStreamParameters> getDefaultCodecStreamParameterList(
            String inputYuvName,
            String outputIvfBaseName,
            int encodeSeconds,
            int[] resolutionScales,
            int frameWidth,
            int frameHeight,
            int frameRate,
            int bitrateMode,
            int[] bitrates,
            boolean syncEncoding) {
        assertTrue(resolutionScales.length <= bitrates.length);
        int numCodecs = resolutionScales.length;
        ArrayList<CodecStreamParameters> outputParameters =
                new ArrayList<CodecStreamParameters>(numCodecs);
        for (int i = 0; i < numCodecs; i++) {
            CodecStreamParameters params = new CodecStreamParameters();
            params.frameWidth = Math.min(frameWidth * resolutionScales[i], 1280);
            params.frameHeight = Math.min(frameHeight * resolutionScales[i], 720);
            if (inputYuvName != null) {
                params.inputYuvFilename = SDCARD_DIR + File.separator + inputYuvName;
            } else {
                params.inputYuvFilename = null;
            }
            params.scaledYuvFilename = SDCARD_DIR + File.separator + outputIvfBaseName +
                    "_" + params.frameWidth + "x" + params.frameHeight + ".yuv";
            params.inputResourceId = R.raw.football_qvga;
            params.encodedIvfFilename = SDCARD_DIR + File.separator + outputIvfBaseName +
                    "_" + params.frameWidth + "x" + params.frameHeight + ".ivf";
            params.outputYuvFilename = SDCARD_DIR + File.separator + outputIvfBaseName +
                    "_" + params.frameWidth + "x" + params.frameHeight + "_out.yuv";
            params.forceSwCodec = false;
            params.frameCount = encodeSeconds * frameRate;
            params.frameRateSet = new int[1];
            params.frameRateSet[0] = frameRate;
            params.bitrateSet = new int[1];
            params.bitrateSet[0] = bitrates[i];
            params.bitrateType = bitrateMode;
            params.temporalLayers = 0;
            params.syncFrameInterval = SYNC_FRAME_INTERVAL;
            params.syncForceFrameInterval = 0;
            if (syncEncoding) {
                params.timeoutDequeue = DEFAULT_TIMEOUT_US;
                params.runInLooperThread = false;
            } else {
                params.timeoutDequeue = 0;
                params.runInLooperThread = true;
                continue; // FIXME add support for async
            }
            outputParameters.add(params);
        }
        return outputParameters;
    }

    protected CodecStreamParameters getDefaultCodecStreamParameters(
            String inputYuvName,
            String outputIvfBaseName,
            int encodeSeconds,
            int frameWidth,
            int frameHeight,
            int frameRate,
            int bitrateMode,
            int bitrate,
            boolean syncEncoding) {
        int[] scaleValues = { 1 };
        int[] bitrates = { bitrate };
        return getDefaultCodecStreamParameterList(
                inputYuvName,
                outputIvfBaseName,
                encodeSeconds,
                scaleValues,
                frameWidth,
                frameHeight,
                frameRate,
                bitrateMode,
                bitrates,
                syncEncoding).get(0);
    }

    public boolean isHwCodecExist() throws Exception {
        CodecProperties propertiesEncoder = getVp8CodecProperties(true, false);
        CodecProperties propertiesDecoder = getVp8CodecProperties(false, false);
        if (propertiesEncoder.isGoogleSwCodec() && propertiesDecoder.isGoogleSwCodec()) {
            return false;
        }
        return true;
    }


    /**
     * Converts (interleaves) YUV420 planar to NV12 (if hw) or NV21 (if sw).
     * Assumes packed, macroblock-aligned frame with no cropping
     * (visible/coded row length == stride).  Swap U/V if |sw|.
     */
    private static byte[] YUV420ToNV(int width, int height, byte[] yuv, boolean sw) {
        byte[] nv = new byte[yuv.length];
        // Y plane we just copy.
        System.arraycopy(yuv, 0, nv, 0, width * height);

        // U & V plane we interleave.
        int u_offset = width * height;
        int v_offset = u_offset + u_offset / 4;
        int nv_offset = width * height;
        if (sw) {
            for (int i = 0; i < width * height / 4; i++) {
                nv[nv_offset++] = yuv[v_offset++];
                nv[nv_offset++] = yuv[u_offset++];
            }
        }
        else {
            for (int i = 0; i < width * height / 4; i++) {
                nv[nv_offset++] = yuv[u_offset++];
                nv[nv_offset++] = yuv[v_offset++];
            }
        }
        return nv;
    }

    /**
     * Converts (de-interleaves) NV12 to YUV420 planar.
     * Stride may be greater than width, slice height may be greater than height.
     */
    private static byte[] NV12ToYUV420(int width, int height,
            int stride, int sliceHeight, byte[] nv12) {
        byte[] yuv = new byte[width * height * 3 / 2];

        // Y plane we just copy.
        for (int i = 0; i < height; i++) {
            System.arraycopy(nv12, i * stride, yuv, i * width, width);
        }

        // U & V plane - de-interleave.
        int u_offset = width * height;
        int v_offset = u_offset + u_offset / 4;
        int nv_offset;
        for (int i = 0; i < height / 2; i++) {
            nv_offset = stride * (sliceHeight + i);
            for (int j = 0; j < width / 2; j++) {
                yuv[u_offset++] = nv12[nv_offset++];
                yuv[v_offset++] = nv12[nv_offset++];
            }
        }
        return yuv;
    }

    private static void imageUpscale1To2(byte[] src, int srcByteOffset, int srcStride,
            byte[] dst, int dstByteOffset, int dstWidth, int dstHeight) {
        for (int i = 0; i < dstHeight/2 - 1; i++) {
            int dstOffset0 = 2 * i * dstWidth + dstByteOffset;
            int dstOffset1 = dstOffset0 + dstWidth;
            int srcOffset0 = i * srcStride + srcByteOffset;
            int srcOffset1 = srcOffset0 + srcStride;
            int pixel00 = (int)src[srcOffset0++] & 0xff;
            int pixel10 = (int)src[srcOffset1++] & 0xff;
            for (int j = 0; j < dstWidth/2 - 1; j++) {
                int pixel01 = (int)src[srcOffset0++] & 0xff;
                int pixel11 = (int)src[srcOffset1++] & 0xff;
                dst[dstOffset0++] = (byte)pixel00;
                dst[dstOffset0++] = (byte)((pixel00 + pixel01 + 1) / 2);
                dst[dstOffset1++] = (byte)((pixel00 + pixel10 + 1) / 2);
                dst[dstOffset1++] = (byte)((pixel00 + pixel01 + pixel10 + pixel11 + 2) / 4);
                pixel00 = pixel01;
                pixel10 = pixel11;
            }
            // last column
            dst[dstOffset0++] = (byte)pixel00;
            dst[dstOffset0++] = (byte)pixel00;
            dst[dstOffset1++] = (byte)((pixel00 + pixel10 + 1) / 2);
            dst[dstOffset1++] = (byte)((pixel00 + pixel10 + 1) / 2);
        }

        // last row
        int dstOffset0 = (dstHeight - 2) * dstWidth + dstByteOffset;
        int dstOffset1 = dstOffset0 + dstWidth;
        int srcOffset0 = (dstHeight/2 - 1) * srcStride + srcByteOffset;
        int pixel00 = (int)src[srcOffset0++] & 0xff;
        for (int j = 0; j < dstWidth/2 - 1; j++) {
            int pixel01 = (int)src[srcOffset0++] & 0xff;
            dst[dstOffset0++] = (byte)pixel00;
            dst[dstOffset0++] = (byte)((pixel00 + pixel01 + 1) / 2);
            dst[dstOffset1++] = (byte)pixel00;
            dst[dstOffset1++] = (byte)((pixel00 + pixel01 + 1) / 2);
            pixel00 = pixel01;
        }
        // the very last pixel - bottom right
        dst[dstOffset0++] = (byte)pixel00;
        dst[dstOffset0++] = (byte)pixel00;
        dst[dstOffset1++] = (byte)pixel00;
        dst[dstOffset1++] = (byte)pixel00;
    }

    /**
    * Up-scale image.
    * Scale factor is defined by source and destination width ratio.
    * Only 1:1 with height crop, 1:2 and 1:4 up-scaling is supported for now.
    * For 640x480 -> 1280x720 conversion only top 640x360 part of the original
    * image is scaled.
    */
    private static byte[] imageScale(byte[] src, int srcWidth, int srcHeight,
            int dstWidth, int dstHeight) throws Exception {
        int srcYSize = srcWidth * srcHeight;
        int dstYSize = dstWidth * dstHeight;
        byte[] dst = null;
        if (dstWidth == srcWidth && dstHeight < srcHeight) {
            // 1:1 scale with height crop
            dst = new byte[dstWidth * dstHeight * 3 / 2];
            System.arraycopy(src, 0, dst, 0, dstYSize);  // Y
            System.arraycopy(src, srcYSize, dst, dstYSize, dstYSize / 4);  // U
            System.arraycopy(src, srcYSize + srcYSize/4, dst, dstYSize + dstYSize/4, dstYSize / 4);
        }
        else if (dstWidth == 2 * srcWidth && dstHeight <= 2 * srcHeight) {
            // 1:2 upscale
            dst = new byte[dstWidth * dstHeight * 3 / 2];
            imageUpscale1To2(src, 0, srcWidth,
                    dst, 0, dstWidth, dstHeight);                                 // Y
            imageUpscale1To2(src, srcYSize, srcWidth / 2,
                    dst, dstYSize, dstWidth / 2, dstHeight / 2);                  // U
            imageUpscale1To2(src, srcYSize * 5 / 4, srcWidth / 2,
                    dst, dstYSize * 5 / 4, dstWidth / 2, dstHeight / 2);          // V
        } else if (dstWidth == 4 * srcWidth && dstHeight <= 4 * srcHeight) {
            // 1:4 upscale - in two steps
            int midWidth = 2 * srcWidth;
            int midHeight = 2 * srcHeight;
            byte[] midBuffer = imageScale(src, srcWidth, srcHeight, midWidth, midHeight);
            dst = imageScale(midBuffer, midWidth, midHeight, dstWidth, dstHeight);

        } else {
            throw new RuntimeException("Can not find proper scaling function");
        }
        return dst;
    }

    /**
     * Packs YUV420 frame by moving it to a smaller size buffer with stride and slice
     * height equal to the original frame width and height.
     */
    private static byte[] PackYUV420(int width, int height,
            int stride, int sliceHeight, byte[] src) {
        byte[] dst = new byte[width * height * 3 / 2];
        // Y copy.
        for (int i = 0; i < height; i++) {
            System.arraycopy(src, i * stride, dst, i * width, width);
        }
        // U and V copy.
        int u_src_offset = stride * sliceHeight;
        int v_src_offset = u_src_offset + u_src_offset / 4;
        int u_dst_offset = width * height;
        int v_dst_offset = u_dst_offset + u_dst_offset / 4;
        for (int i = 0; i < height / 2; i++) {
            System.arraycopy(src, u_src_offset + i * (stride / 2),
                    dst, u_dst_offset + i * (width / 2), width / 2);
            System.arraycopy(src, v_src_offset + i * (stride / 2),
                    dst, v_dst_offset + i * (width / 2), width / 2);
        }
        return dst;
    }

    private void cacheScaledImage(
            String srcYuvFilename, int srcResourceId,
            int srcFrameWidth, int srcFrameHeight,
            String dstYuvFilename,
            int dstFrameWidth, int dstFrameHeight,
            int dstColor) throws Exception {
        InputStream srcStream = OpenFileOrResourceId(srcYuvFilename, srcResourceId);
        int srcFrameSize = srcFrameWidth * srcFrameHeight * 3 / 2;
        byte[] srcFrame = new byte[srcFrameSize];
        byte[] dstFrame = null;
        Log.d(TAG, "Scale to " + dstFrameWidth + " x " + dstFrameHeight +
                ". Clr: 0x" + Integer.toHexString(dstColor) + " -> " + dstYuvFilename);
        // Check if scaling has already been done in previous tests
        // and we have file of expected size on sdcard.
        boolean runScaling = true;
        int inputFrames = srcStream.available() / srcFrameSize;
        int expectedFileSize = inputFrames * dstFrameWidth * dstFrameHeight * 3 / 2;
        try {
            FileInputStream existingDstFile =  new FileInputStream(dstYuvFilename);
            int outputFileSize = existingDstFile.available();
            existingDstFile.close();
            if (outputFileSize == expectedFileSize) {
                Log.d(TAG, "No scalig required - output file already exist.");
                runScaling = false;
            }
        } catch (Exception e) {
        }

        if (runScaling) {
            FileOutputStream dstFile = new FileOutputStream(dstYuvFilename, false);
            while (true) {
                int bytesRead = srcStream.read(srcFrame);
                if (bytesRead != srcFrame.length) {
                    break;
                }
                if (dstFrameWidth == srcFrameWidth && dstFrameHeight == srcFrameHeight) {
                    dstFrame = srcFrame;
                } else {
                    dstFrame = imageScale(srcFrame, srcFrameWidth, srcFrameHeight,
                            dstFrameWidth, dstFrameHeight);
                }
                dstFile.write(dstFrame);
            }
            dstFile.close();
        }
        srcStream.close();
    }

    private boolean isResolutionSupported(int width, int height, boolean isEncoder) {
        try {
            CodecProperties properties = getVp8CodecProperties(isEncoder, false);
            int flags = 0;
            MediaFormat format = MediaFormat.createVideoFormat(VP8_MIME, width, height);
            format.setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
            if (isEncoder) {
                format.setInteger(MediaFormat.KEY_BIT_RATE, 1000000);
                format.setInteger(MediaFormat.KEY_FRAME_RATE, 30);
                flags = MediaCodec.CONFIGURE_FLAG_ENCODE;
            }
            MediaCodec codec = MediaCodec.createByCodecName(properties.codecName);
            codec.configure(format, null, null, flags);
            codec.start();
            codec.stop();
            codec.release();
        } catch (Exception e) {
            Log.e(TAG, "Can not create codec for resolution " + width + " x " + height);
            return false;
        }
        return true;
    }

    public int[] supportedScaleValues(int width, int height, int[] scales, boolean isEncoder) {
        ArrayList<Integer> scalesList = new ArrayList<Integer>();
        for (int scale : scales) {
            int frameWidth = Math.min(width * scale, 1280);
            int frameHeight = Math.min(height * scale, 720);
            if (isResolutionSupported(frameWidth, frameHeight, isEncoder)) {
                scalesList.add(scale);
            }
        }
        int[] outScales = new int[scalesList.size()];
        for (int i = 0; i < scalesList.size(); i++) {
            outScales[i] = scalesList.get(i);
        }
        return outScales;
    }

    /**
     * Helper function to return InputStream from either filename (if set)
     * or resource id (if filename is not set).
     */
    private InputStream OpenFileOrResourceId(String filename, int resourceId) throws Exception {
        if (filename != null) {
            return new FileInputStream(filename);
        }
        return mResources.openRawResource(resourceId);
    }

    /**
     * Results of frame encoding.
     */
    protected class MediaCodecOutput {
        public long inPresentationTimeUs;
        public long outPresentationTimeUs;
        public int inputRtcTimeUs;
        public int outputRtcTimeUs;
        public int colorFormat;
        public int stride;
        public int sliceHeight;
        public boolean outputGenerated;
        public int flags;
        public byte[] buffer;
    }

    /**
     * Media codec wrapper class.
     * Allows to run the encoder/decoder either in a callee's thread or in a looper thread
     * using buffer dequeue ready notification callbacks.
     *
     * Function feedInput() is used to send raw video frame to the codec input. When codec
     * is configured to run in async mode the function will run in a looper thread.
     * Output frame can be retrieved by calling getOutput() function.
     */
    protected class MediaCodecAsync extends Thread /* FIXME implements MediaCodec.NotificationCallback */ {
        private int mId;
        private String mIdString;
        private MediaCodec mCodec;
        private MediaFormat mFormat;
        private ByteBuffer[] mInputBuffers;
        private ByteBuffer[] mOutputBuffers;
        private int mInputFrameIndex;
        private int mOutputFrameIndex;
        private int mInputBufIndex;
        private long mTimeout;
        private MediaCodec.BufferInfo mBufferInfo;
        private long mInPresentationTimeUs;
        private long mOutPresentationTimeUs;
        private boolean mAsync;
        private boolean mIsEncoder;
        // Flag indicating if input frame was consumed by the encoder in feedInput() call.
        private boolean mConsumedInput;
        // Result of frame encoding/decoding returned by getOutput() call.
        private MediaCodecOutput mOutput;
        // Object used to signal that looper thread has started and Handler instance associated
        // with looper thread has been allocated.
        private final Object mThreadEvent = new Object();
        // Object used to signal that MediaCodec buffer dequeue notification callback
        // was received.
        private final Object mCallbackEvent = new Object();
        private Handler mHandler;
        private boolean mCallbackReceived;
        // Frame width, height, stride, slice height and color format received
        // during INFO_OUTPUT_FORMAT_CHANGED message
        private int mWidth;
        private int mHeight;
        private int mStride;
        private int mSliceHeight;
        private int mColorFormat;
        // Profiling variables
        private long mStartRtcTimeNs; // codec start time in ns
        private ArrayList<Integer> mInputRtcTimeUs;  // time at which frame was fed to codec
        // Enable reading data from codec output
        public boolean mOutputDataEnable = true;


        /* FIXME @Override */
        public void onCodecNotify(MediaCodec codec) {
            synchronized (mCallbackEvent) {
                mCallbackReceived = true;
                mCallbackEvent.notify();
            }
            return;
        }

        private synchronized void requestStart() throws Exception {
            mHandler = null;
            start();
            // Wait for Hander allocation
            synchronized (mThreadEvent) {
                while (mHandler == null) {
                    mThreadEvent.wait();
                }
            }
        }

        @Override
        public void run() {
            Looper.prepare();
            synchronized (mThreadEvent) {
                mHandler = new Handler();
                mThreadEvent.notify();
            }
            Looper.loop();
        }

        private void runCallable(final Callable<?> callable) throws Exception {
            if (mAsync) {
                final Exception[] exception = new Exception[1];
                final CountDownLatch countDownLatch = new CountDownLatch(1);
                mHandler.post( new Runnable() {
                    @Override
                    public void run() {
                        try {
                            callable.call();
                        } catch (Exception e) {
                            exception[0] = e;
                        } finally {
                            countDownLatch.countDown();
                        }
                    }
                } );

                // Wait for task completion
                countDownLatch.await();
                if (exception[0] != null) {
                    throw exception[0];
                }
            } else {
                callable.call();
            }
        }

        private synchronized void requestStop() throws Exception {
            mHandler.post( new Runnable() {
                @Override
                public void run() {
                    // This will run on the Looper thread
                    Log.v(TAG, "MediaCodec looper quitting");
                    Looper.myLooper().quitSafely();
                }
            } );
            // Wait for completion
            join();
            mHandler = null;
        }

        private void createCodecInternal(final String name,
                final MediaFormat format, final long timeout)
                throws Exception {
            mBufferInfo = new MediaCodec.BufferInfo();
            mFormat = format;
            mTimeout = timeout;
            mInputFrameIndex = 0;
            mOutputFrameIndex = 0;
            mInPresentationTimeUs = 0;
            mOutPresentationTimeUs = 0;
            mStartRtcTimeNs = -1;
            mInputRtcTimeUs = new ArrayList<Integer>();

            mWidth = format.getInteger(MediaFormat.KEY_WIDTH);
            mStride = mWidth;
            mHeight = format.getInteger(MediaFormat.KEY_HEIGHT);
            mSliceHeight = mHeight;
            mColorFormat = format.getInteger(MediaFormat.KEY_COLOR_FORMAT);
            int flags;
            if (mIsEncoder) {
                mIdString = "Enc" + mId;
                flags = MediaCodec.CONFIGURE_FLAG_ENCODE;
            } else {
                mIdString = "Dec" + mId;
                flags = 0;
            }

            mCodec = MediaCodec.createByCodecName(name);
            mCodec.configure(mFormat, null, null, flags);
            mCodec.start();
            if (mAsync) {
                /* FIXME mCodec.setNotificationCallback(this); */
            }
            mInputBuffers = mCodec.getInputBuffers();
            mOutputBuffers = mCodec.getOutputBuffers();
            Log.d(TAG, "Input buffers: " + mInputBuffers.length +
                ". Output buffers: " + mOutputBuffers.length);
        }


        public void createCodec(int id, final String name, final MediaFormat format,
                final long timeout, boolean flagEncode, boolean async)  throws Exception {
            mId = id;
            mAsync = async;
            mIsEncoder = flagEncode;
            if (mAsync) {
                requestStart(); // start looper thread
            }
            runCallable( new Callable<Void>() {
                @Override
                public Void call() throws Exception {
                    createCodecInternal(name, format, timeout);
                    return null;
                }
            } );
        }

        private void feedInputInternal(final byte[] frame, final int timestamp,
                final boolean inputEOS) {
            if (mStartRtcTimeNs < 0) {
                mStartRtcTimeNs = SystemClock.elapsedRealtimeNanos();
            }
            int inputRtcTimeUs =
                    (int)((SystemClock.elapsedRealtimeNanos() - mStartRtcTimeNs) / 1000);
            mConsumedInput = false;
            // Feed input
            mInputBufIndex = mCodec.dequeueInputBuffer(mTimeout);

            if (mInputBufIndex >= 0) {
                mInputRtcTimeUs.add(inputRtcTimeUs);
                ByteBuffer frameBuffer = ByteBuffer.wrap(frame);
                mInputBuffers[mInputBufIndex].clear();
                mInputBuffers[mInputBufIndex].put(frameBuffer);
                mInputBuffers[mInputBufIndex].rewind();
                int frameLength = frame.length;
                int flags = 0;
                if (inputEOS) {
                    Log.d(TAG, "---" + mIdString + " input EOS for frame # " + mInputFrameIndex);
                    flags = MediaCodec.BUFFER_FLAG_END_OF_STREAM;
                    if (mIsEncoder) {
                        frameLength = 0;
                    }
                }
                if (frameLength > 0) {
                    mInPresentationTimeUs = timestamp;
                    Log.v(TAG, mIdString + ". Frame in # " + mInputFrameIndex +
                            ". Size: " + frameLength + ". InTime: " +
                            ((mInPresentationTimeUs + 500)/1000));
//                            ". Tin: " + inputRtcTimeUs);
                    mInputFrameIndex++;
                }

                mCodec.queueInputBuffer(
                        mInputBufIndex,
                        0,  // offset
                        frameLength,  // size
                        mInPresentationTimeUs,
                        flags);

                mConsumedInput = true;
            }
            mCallbackReceived = false;
        }

        public boolean feedInput(final byte[] encFrame, final int timestamp,
                final boolean inputEOS) throws Exception {
            runCallable( new Callable<Void>() {
                @Override
                public Void call() throws Exception {
                    feedInputInternal(encFrame, timestamp, inputEOS);
                    return null;
                }
            } );
            return mConsumedInput;
        }

        private void getOutputInternal() {
            mOutput = new MediaCodecOutput();
            mOutput.inPresentationTimeUs = mInPresentationTimeUs;
            mOutput.outPresentationTimeUs = mOutPresentationTimeUs;
            mOutput.outputGenerated = false;

            // Get output from the encoder
            int result = mCodec.dequeueOutputBuffer(mBufferInfo, mTimeout);
            while (result == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED ||
                    result == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                if (result == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
                    mOutputBuffers = mCodec.getOutputBuffers();
                } else if (result == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
                    mFormat = mCodec.getOutputFormat();
                    Log.d(TAG, "Format changed: " + mFormat.toString());
                    mWidth = mFormat.getInteger(MediaFormat.KEY_WIDTH);
                    mHeight = mFormat.getInteger(MediaFormat.KEY_HEIGHT);
                    if (mFormat.containsKey(MediaFormat.KEY_COLOR_FORMAT)) {
                        mColorFormat = mFormat.getInteger(MediaFormat.KEY_COLOR_FORMAT);
                    }
                    Log.d(TAG, "Color: 0x" + Integer.toHexString(mColorFormat));

                    if (mFormat.containsKey("stride")) {
                        mStride = mFormat.getInteger("stride");
                    }
                    if (mFormat.containsKey("slice-height")) {
                        mSliceHeight = mFormat.getInteger("slice-height");
                    }
                    Log.d(TAG, "Frame stride and slice height: " + mStride + " x " + mSliceHeight);
                    mStride = Math.max(mWidth, mStride);
                    mSliceHeight = Math.max(mHeight, mSliceHeight);
                }
                result = mCodec.dequeueOutputBuffer(mBufferInfo, mTimeout);
            }

            if (result >= 0) {
                int outputBufIndex = result;
                mOutput.buffer = new byte[mBufferInfo.size];
                if (mOutputDataEnable && mBufferInfo.size > 0) {
                    mOutputBuffers[outputBufIndex].position(mBufferInfo.offset);
                    mOutputBuffers[outputBufIndex].get(mOutput.buffer, 0, mBufferInfo.size);
                }
                //Log.v(TAG, mOutput.buffer[0] + " " + mOutput.buffer[1] + " " + mOutput.buffer[2] + " " + mOutput.buffer[3] + " " + mOutput.buffer[4] + " " + mOutput.buffer[5]);
                mOutPresentationTimeUs = mBufferInfo.presentationTimeUs;
                if (mInputRtcTimeUs.size() > 0) {
                    mOutput.inputRtcTimeUs = mInputRtcTimeUs.remove(0);
                }
                mOutput.outputRtcTimeUs =
                        (int)((SystemClock.elapsedRealtimeNanos() - mStartRtcTimeNs) / 1000);

                String logStr = mIdString + ". Frame # " + mOutputFrameIndex;
                if ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != 0) {
                    logStr += " CONFIG. ";
                }
                if ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_SYNC_FRAME) != 0) {
                    logStr += " KEY. ";
                }
                logStr += " Size: " + mBufferInfo.size;
                logStr += ". InTime: " + (mInPresentationTimeUs + 500)/1000 +
                        ". OutTime: " + (mOutPresentationTimeUs + 500)/1000;
                //logStr += " TIn: " + mOutput.inputRtcTimeUs + ". Tout: " + mOutput.outputRtcTimeUs;
                //        ". TLat: " + (mOutput.outputRtcTimeUs - mOutput.inputRtcTimeUs);
                Log.v(TAG, logStr);
                if ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    Log.d(TAG, "---" + mIdString + " output EOS for frame # " + mOutputFrameIndex);
                }
                /*if (mIsEncoder && mOutputFrameIndex == 0 &&
                        ((mBufferInfo.flags & MediaCodec.BUFFER_FLAG_SYNC_FRAME) == 0) ) {
                    throw new RuntimeException("First frame is not a sync frame.");
                }*/
                mCodec.releaseOutputBuffer(outputBufIndex, false);

                if (mBufferInfo.size > 0) {
                    mOutputFrameIndex++;
                    mOutput.outPresentationTimeUs = mOutPresentationTimeUs;
                }
                mOutput.flags = mBufferInfo.flags;
                if (!mIsEncoder) {
                    mOutput.colorFormat = mColorFormat;
                    mOutput.stride = mStride;
                    mOutput.sliceHeight = mSliceHeight;
                }
                mOutput.outputGenerated = true;
            }
            mCallbackReceived = false;
        }

        public MediaCodecOutput getOutput() throws Exception {
            runCallable( new Callable<Void>() {
                @Override
                public Void call() throws Exception {
                    getOutputInternal();
                    return null;
                }
            } );
            return mOutput;
        }

        public void forceSyncFrame() throws Exception {
            final Bundle syncFrame = new Bundle();
            syncFrame.putInt(MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME, 0);
            runCallable( new Callable<Void>() {
                @Override
                public Void call() throws Exception {
                    mCodec.setParameters(syncFrame);
                    return null;
                }
            } );
        }

        public void updateBitrate(int bitrate) throws Exception {
            final Bundle bitrateUpdate = new Bundle();
            bitrateUpdate.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, bitrate);
            runCallable( new Callable<Void>() {
                @Override
                public Void call() throws Exception {
                    mCodec.setParameters(bitrateUpdate);
                    return null;
                }
            } );
        }

        public void waitForBufferEvent() throws Exception {
            if (mAsync) {
                synchronized (mCallbackEvent) {
                    if (!mCallbackReceived) {
                        mCallbackEvent.wait(1000); // wait 1 sec for a callback
                        // throw an exception if callback was not received
                        if (!mCallbackReceived) {
                            throw new RuntimeException("MediaCodec callback was not received");
                        }
                    }
                }
            } else {
                Thread.sleep(2);
            }
        }

        public void deleteCodec() throws Exception {
            runCallable( new Callable<Void>() {
                @Override
                public Void call() throws Exception {
                    mCodec.stop();
                    mCodec.release();
                    return null;
                }
            } );
            if (mAsync) {
                requestStop(); // Stop looper thread
            }
        }
    }

    /**
     * Extended per buffer metadata includes an offset and size specifying
     * the range of valid data in the associated codec buffer.
     */
    public final static class BufferInfo {
        public void set(
                int newSize, long newTimeUs, long newTimeUsDelta,
                int newInputRtcTimeUs, int newOutputRtcTimeUs, int newFlags) {
            size = newSize;
            presentationTimeUs = newTimeUs;
            presentationTimeUsDelta = newTimeUsDelta;
            inputRtcTimeUs = newInputRtcTimeUs;
            outputRtcTimeUs = newOutputRtcTimeUs;
            flags = newFlags;
        }

        public int size;
        public long presentationTimeUs;
        public long presentationTimeUsDelta;
        public int inputRtcTimeUs;
        public int outputRtcTimeUs;
        public int flags;
    }

    /**
     * Vp8 decoding loop supporting decoding single streams with an option
     * to run in a looper thread and use buffer ready notification callbacks.
     *
     * Input ivf stream is described by streamParams parameters.
     *
     * MediaCodec will raise an IllegalStateException
     * whenever vp8 decoder fails to decode a frame.
     *
     * @param streamParams  Structure with codec stream parameters
     * @return              Returns array of decoded frames information for each frame.
     */
    protected ArrayList<BufferInfo> decode(
            CodecStreamParameters streamParams) throws Exception {
        ArrayList<BufferInfo> bufferInfos = new ArrayList<BufferInfo>();
        CodecProperties properties = getVp8CodecProperties(false, streamParams.forceSwCodec);
        // Open input/output.
        IvfReader ivf = new IvfReader(streamParams.encodedIvfFilename);
        int frameWidth = ivf.getWidth();
        int frameHeight = ivf.getHeight();
        int frameCount = ivf.getFrameCount();
        assertTrue(frameWidth > 0);
        assertTrue(frameHeight > 0);
        assertTrue(frameCount > 0);

        FileOutputStream yuv = null;
        if (streamParams.outputYuvFilename != null) {
            yuv = new FileOutputStream(streamParams.outputYuvFilename, false);
        }

        // Create decoder.
        MediaFormat format = MediaFormat.createVideoFormat(VP8_MIME,
                                                           ivf.getWidth(),
                                                           ivf.getHeight());
        format.setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
        Log.d(TAG, "Creating decoder " + properties.codecName +
                ". Color format: 0x" + Integer.toHexString(properties.colorFormat) +
                ". " + frameWidth + " x " + frameHeight);
        Log.d(TAG, "  Format: " + format);
        Log.d(TAG, "  In: " + streamParams.encodedIvfFilename +
                ". Out:" + streamParams.outputYuvFilename);

        MediaCodecAsync codec = new MediaCodecAsync();
        codec.createCodec(0, properties.codecName, format, streamParams.timeoutDequeue,
                false, streamParams.runInLooperThread);
        if (yuv == null) {
            codec.mOutputDataEnable = false; // no need to memcpy output if it will not be saved
        }

        // decode loop
        int inputFrameIndex = 0;
        boolean sawOutputEOS = false;
        boolean sawInputEOS = false;
        boolean consumedInputEOS = false; // EOS flag is consumed by encoder
        boolean inputConsumed = true;
        double presentationTimeUsCurrent = 0;
        byte[] frame = null;
        int frameIndexInc = Math.max(1,  streamParams.decodingRateDecimator);

        while (!sawOutputEOS) {

            // Read and feed input frame
            if (!consumedInputEOS) {
                // Read new input buffers - if previous input was consumed and no EOS
                if (inputConsumed && !sawInputEOS) {
                    frame = ivf.readFrame(inputFrameIndex);
                    // timestamp - sec -> us
                    presentationTimeUsCurrent = ivf.getFrameTimestamp(inputFrameIndex) * 1e6;
                    // Check EOS
                    if (inputFrameIndex + frameIndexInc >= frameCount) {
                        sawInputEOS = true;
                    }
                }

                inputConsumed = codec.feedInput(frame,
                        (int)(presentationTimeUsCurrent + 0.5), sawInputEOS);
                if (inputConsumed) {
                    inputFrameIndex += frameIndexInc;
                    consumedInputEOS = sawInputEOS;
                }
            }


            // Get output from the decoder
            MediaCodecOutput out = codec.getOutput();
            if (out.outputGenerated) {
                // Detect output EOS
                if ((out.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    sawOutputEOS = true;
                }

                if (out.buffer.length > 0) {
                    // Save decoder output to yuv file.
                    if (yuv != null) {
                        // Convert NV12 to YUV420 if necessary
                        if (out.colorFormat != CodecCapabilities.COLOR_FormatYUV420Planar) {
                            out.buffer = NV12ToYUV420(frameWidth, frameHeight,
                                    out.stride, out.sliceHeight, out.buffer);
                        }
                        int writeLength = Math.min(frameWidth * frameHeight * 3 / 2,
                                out.buffer.length);
                        // Pack frame if necessary.
                        if (writeLength < out.buffer.length &&
                                (out.stride > frameWidth || out.sliceHeight > frameHeight)) {
                            out.buffer = PackYUV420(frameWidth, frameHeight,
                                    out.stride, out.sliceHeight, out.buffer);
                        }
                        yuv.write(out.buffer, 0, writeLength);
                    }

                    // Update statistics - store presentation time delay in offset
                    long presentationTimeUsDelta = out.inPresentationTimeUs -
                            out.outPresentationTimeUs;
                    BufferInfo bufferInfoCopy = new BufferInfo();
                    bufferInfoCopy.set(out.buffer.length,
                            out.outPresentationTimeUs, presentationTimeUsDelta,
                            out.inputRtcTimeUs, out.outputRtcTimeUs, out.flags);
                    bufferInfos.add(bufferInfoCopy);
                }
            }

            // If codec is not ready to accept input/output - wait for buffer ready callback
            if ((!inputConsumed || consumedInputEOS) && !out.outputGenerated) {
                codec.waitForBufferEvent();
            }
        }

        codec.deleteCodec();
        ivf.close();
        if (yuv != null) {
            yuv.close();
        }

        return bufferInfos;
    }

    /**
     * Vp8 decoding loop supporting decoding multiple parallel streams with an option
     * to run in a looper thread and use buffer ready notification callbacks.
     *
     * Input ivf stream is described by streamParams parameters.
     *
     * @param streamParams  Structure with codec stream parameters
     * @return              Returns array of decoded frames information for each frame.
     */
    protected ArrayList<ArrayList<BufferInfo>> decodeParallel(
            ArrayList<CodecStreamParameters> streamParams) throws Exception {
        int numCodecs = streamParams.size();

        // Create arrays of input/output, formats, bitrates etc
        ArrayList<ArrayList<BufferInfo>> bufferInfos =
                new ArrayList<ArrayList<BufferInfo>>(numCodecs);
        IvfReader[] ivf = new IvfReader[numCodecs];
        int frameWidth[] = new int[numCodecs];
        int frameHeight[] = new int[numCodecs];
        int frameCount[] = new int[numCodecs];
        FileOutputStream[] yuv = new FileOutputStream[numCodecs];
        MediaFormat[] format = new MediaFormat[numCodecs];
        MediaCodecAsync[] codec = new MediaCodecAsync[numCodecs];
        boolean[] sawOutputEOS = new boolean[numCodecs];
        boolean[] sawInputEOS = new boolean[numCodecs];
        boolean[] consumedInputEOS = new boolean[numCodecs]; // EOS flag is consumed by codec
        boolean[] bufferConsumed = new boolean[numCodecs];
        boolean[] inputConsumed = new boolean[numCodecs];
        int[] inputFrameIndex = new int[numCodecs];
        double[] presentationTimeUsCurrent = new double[numCodecs];
        byte[][] srcFrame = new byte[numCodecs][];
        boolean sawOutputEOSTotal = false;
        boolean bufferConsumedTotal = false;

        for (int i = 0; i < numCodecs; i++) {
            CodecStreamParameters params = streamParams.get(i);
            CodecProperties properties = getVp8CodecProperties(false, params.forceSwCodec);

            // Create buffer info storage
            bufferInfos.add(new ArrayList<BufferInfo>());

            // Open input/output.
            ivf[i] = new IvfReader(params.encodedIvfFilename);
            frameWidth[i] = ivf[i].getWidth();
            frameHeight[i] = ivf[i].getHeight();
            frameCount[i] = ivf[i].getFrameCount();
            assertTrue(frameWidth[i] > 0);
            assertTrue(frameHeight[i] > 0);
            assertTrue(frameCount[i] > 0);

            yuv[i] = null;
            if (params.outputYuvFilename != null) {
                yuv[i] = new FileOutputStream(params.outputYuvFilename, false);
            }

            // Create decoder.
            format[i] = MediaFormat.createVideoFormat(VP8_MIME, frameWidth[i], frameHeight[i]);
            format[i].setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
            Log.d(TAG, "Creating decoder #" + i + ": " + properties.codecName +
                    ". Color format: 0x" + Integer.toHexString(properties.colorFormat) +
                    ". " + frameWidth[i] + " x " + frameHeight[i]);
            Log.d(TAG, "  Format: " + format[i]);
            Log.d(TAG, "  Input  ivf: " + params.encodedIvfFilename);
            Log.d(TAG, "  Output yuv: " + params.outputYuvFilename);

            codec[i] = new MediaCodecAsync();
            codec[i].createCodec(i, properties.codecName, format[i],
                    params.timeoutDequeue, false, params.runInLooperThread);

            inputConsumed[i] = true;
            srcFrame[i] = null;
        }

        // decode loop
        while (!sawOutputEOSTotal) {
            // Feed input buffer to all decoders
            for (int i = 0; i < numCodecs; i++) {
                bufferConsumed[i] = false;
                if (consumedInputEOS[i]) {
                    continue;
                }

                // Read new input buffers - if previous input was consumed and no EOS
                if (inputConsumed[i] && !sawInputEOS[i]) {
                    srcFrame[i] = ivf[i].readFrame(inputFrameIndex[i]);
                    // timestamp - sec -> us
                    presentationTimeUsCurrent[i] =
                            ivf[i].getFrameTimestamp(inputFrameIndex[i]) * 1e6;
                    // Check EOS
                    if (inputFrameIndex[i] == frameCount[i] - 1) {
                        sawInputEOS[i] = true;
                    }
                }

                inputConsumed[i] = codec[i].feedInput(srcFrame[i],
                        (int)(presentationTimeUsCurrent[i] + 0.5), sawInputEOS[i]);

                if (inputConsumed[i]) {
                    inputFrameIndex[i]++;
                    consumedInputEOS[i] = sawInputEOS[i];
                    bufferConsumed[i] = true;
                }
            }

            // Get output from all decoders
            for (int i = 0; i < numCodecs; i++) {
                if (sawOutputEOS[i]) {
                    continue;
                }

                MediaCodecOutput out = codec[i].getOutput();
                if (out.outputGenerated) {
                    bufferConsumed[i] = true;
                    // Detect output EOS
                    if ((out.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                        sawOutputEOS[i] = true;
                    }
                    if (out.buffer.length > 0) {
                        // Save decoder output to yuv file.
                        if (yuv[i] != null) {
                            // Convert NV12 to YUV420 if necessary
                            if (out.colorFormat != CodecCapabilities.COLOR_FormatYUV420Planar) {
                                out.buffer = NV12ToYUV420(frameWidth[i], frameHeight[i],
                                        out.stride, out.sliceHeight, out.buffer);
                            }
                            int writeLength = Math.min(frameWidth[i] * frameHeight[i] * 3 / 2,
                                    out.buffer.length);
                            // Pack frame if necessary.
                            if (writeLength < out.buffer.length && (out.stride > frameWidth[i] ||
                                    out.sliceHeight > frameHeight[i])) {
                                out.buffer = PackYUV420(frameWidth[i], frameHeight[i],
                                        out.stride, out.sliceHeight, out.buffer);
                            }
                            yuv[i].write(out.buffer, 0, writeLength);
                        }

                        // Update statistics - store presentation time delay in offset
                        long presentationTimeUsDelta = out.inPresentationTimeUs -
                                out.outPresentationTimeUs;
                        BufferInfo bufferInfoCopy = new BufferInfo();
                        bufferInfoCopy.set(out.buffer.length,
                                out.outPresentationTimeUs, presentationTimeUsDelta,
                                out.inputRtcTimeUs, out.outputRtcTimeUs, out.flags);
                        bufferInfos.get(i).add(bufferInfoCopy);
                    }
                }
            }

            // If codec is not ready to accept input/output - wait for buffer ready callback
            bufferConsumedTotal = false;
            for (boolean bufferConsumedCurrent : bufferConsumed) {
                bufferConsumedTotal |= bufferConsumedCurrent;
            }
            if (!bufferConsumedTotal) {
                // Pick the decoder to wait for
                for (int i = 0; i < numCodecs; i++) {
                    if (!bufferConsumed[i] && !sawOutputEOS[i]) {
                        codec[i].waitForBufferEvent();
                        break;
                    }
                }
            }

            // Check if EOS happened for all decoders
            sawOutputEOSTotal = true;
            for (boolean sawOutputEOSStream : sawOutputEOS) {
                sawOutputEOSTotal &= sawOutputEOSStream;
            }
        }

        for (int i = 0; i < numCodecs; i++) {
            codec[i].deleteCodec();
            ivf[i].close();
            if (yuv[i] != null) {
                yuv[i].close();
            }
        }

        return bufferInfos;
    }


    /**
     * Vp8 encoding loop supporting encoding single streams with an option
     * to run in a looper thread and use buffer ready notification callbacks.
     *
     * Output stream is described by streamParams parameters.
     *
     * MediaCodec will raise an IllegalStateException
     * whenever vp8 encoder fails to encode a frame.
     *
     * Color format of input file should be YUV420, and frameWidth,
     * frameHeight should be supplied correctly as raw input file doesn't
     * include any header data.
     *
     * @param streamParams  Structure with codec stream parameters
     * @return              Returns array of encoded frames information for each frame.
     */
    protected ArrayList<BufferInfo> encode(
            CodecStreamParameters streamParams) throws Exception {

        ArrayList<BufferInfo> bufferInfos = new ArrayList<BufferInfo>();
        CodecProperties properties = getVp8CodecProperties(true, streamParams.forceSwCodec);
        Log.d(TAG, "Source reslution: " + streamParams.frameWidth + " x " +
                streamParams.frameHeight);
        int bitrate = streamParams.bitrateSet[0];
        int framerate = streamParams.frameRateSet[0];

        // Open input/output
        InputStream yuvStream = OpenFileOrResourceId(
                streamParams.inputYuvFilename, streamParams.inputResourceId);
        IvfWriter ivf = null;
        if (streamParams.encodedIvfFilename != null) {
            ivf = new IvfWriter(streamParams.encodedIvfFilename,
                    streamParams.frameWidth, streamParams.frameHeight);
        }

        // Create a media format signifying desired output.
        MediaFormat format = MediaFormat.createVideoFormat(
                VP8_MIME, streamParams.frameWidth, streamParams.frameHeight);
        format.setInteger(MediaFormat.KEY_BIT_RATE, bitrate);
        if (streamParams.bitrateType == VIDEO_ControlRateConstant) {
            format.setInteger("bitrate-mode", VIDEO_ControlRateConstant); // set CBR
        }
        if (streamParams.temporalLayers == 1) {
            format.setString("ts-schema", "webrtc.vp8.1-layer");
        } else if (streamParams.temporalLayers == 2) {
            format.setString("ts-schema", "webrtc.vp8.2-layer");
        } else if (streamParams.temporalLayers == 3) {
            format.setString("ts-schema", "webrtc.vp8.3-layer");
        }
        format.setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
        format.setInteger(MediaFormat.KEY_FRAME_RATE, framerate);
        int syncFrameInterval = (streamParams.syncFrameInterval + framerate / 2) / framerate;
        format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, syncFrameInterval);

        // Create encoder
        Log.d(TAG, "Creating encoder " + properties.codecName +
                ". Color format: 0x" + Integer.toHexString(properties.colorFormat)+ " : " +
                streamParams.frameWidth + " x " + streamParams.frameHeight +
                ". Bitrate: " + bitrate + " Bitrate type: " + streamParams.bitrateType +
                ". Fps:" + framerate + ". TS Layers: " + streamParams.temporalLayers +
                ". Key frame:" + syncFrameInterval * framerate +
                ". Force keyFrame: " + streamParams.syncForceFrameInterval);
        Log.d(TAG, "  Format: " + format);
        Log.d(TAG, "  Output ivf:" + streamParams.encodedIvfFilename);
        MediaCodecAsync codec = new MediaCodecAsync();
        codec.createCodec(0, properties.codecName, format,
                streamParams.timeoutDequeue, true, streamParams.runInLooperThread);
        if (ivf == null) {
            codec.mOutputDataEnable = false; // no need to memcpy output if it will not be saved
        }

        // encode loop
        boolean sawInputEOS = false;  // no more data
        boolean consumedInputEOS = false; // EOS flag is consumed by encoder
        boolean sawOutputEOS = false;
        boolean inputConsumed = true;
        int inputFrameIndex = 0;
        int lastBitrate = bitrate;
        int lastFramerate = framerate;
        int srcFrameSize = streamParams.frameWidth * streamParams.frameHeight * 3 / 2;
        byte[] srcFrame = new byte[srcFrameSize];
        double frameDuration = 1000000.0 / framerate;
        double presentationTimeUsCurrent = 0;

        while (!sawOutputEOS) {

            // Read and feed input frame
            if (!consumedInputEOS) {

                // Read new input buffers - if previous input was consumed and no EOS
                if (inputConsumed && !sawInputEOS) {
                    int bytesRead = yuvStream.read(srcFrame);

                    // Check EOS
                    if (streamParams.frameCount > 0 && inputFrameIndex >= streamParams.frameCount) {
                        sawInputEOS = true;
                    }

                    if (!sawInputEOS && bytesRead == -1) {
                        if (streamParams.frameCount == 0) {
                            sawInputEOS = true;
                        } else {
                            yuvStream.close();
                            yuvStream = OpenFileOrResourceId(
                                    streamParams.inputYuvFilename, streamParams.inputResourceId);
                            bytesRead = yuvStream.read(srcFrame);
                        }
                    }

                    // Force sync frame if syncForceFrameinterval is set.
                    if (!sawInputEOS && inputFrameIndex > 0 &&
                            streamParams.syncForceFrameInterval > 0 &&
                            (inputFrameIndex % streamParams.syncForceFrameInterval) == 0) {
                        Log.d(TAG, "---Requesting sync frame # " + inputFrameIndex);
                        codec.forceSyncFrame();
                    }

                    // Dynamic bitrate change.
                    if (!sawInputEOS && streamParams.bitrateSet.length > inputFrameIndex) {
                        int newBitrate = streamParams.bitrateSet[inputFrameIndex];
                        if (newBitrate != lastBitrate) {
                            Log.d(TAG, "--- Requesting new bitrate " + newBitrate +
                                    " for frame " + inputFrameIndex);
                            codec.updateBitrate(newBitrate);
                            lastBitrate = newBitrate;
                        }
                    }

                    // Dynamic framerate change
                    if (!sawInputEOS && streamParams.frameRateSet.length > inputFrameIndex) {
                        int newFramerate = streamParams.frameRateSet[inputFrameIndex];
                        if (newFramerate != lastFramerate) {
                            Log.d(TAG, "--- Requesting new framerate " + newFramerate +
                                    " for frame " + inputFrameIndex);
                            lastFramerate = newFramerate;
                            frameDuration = 1000000.0 / newFramerate;
                        }
                    }

                    // Convert YUV420 to NV12 if necessary
                    if (properties.colorFormat != CodecCapabilities.COLOR_FormatYUV420Planar) {
                        srcFrame = YUV420ToNV(streamParams.frameWidth, streamParams.frameHeight,
                                srcFrame, properties.isGoogleSwCodec());
                    }

                }

                inputConsumed = codec.feedInput(srcFrame,
                        (int)(presentationTimeUsCurrent + 0.5), sawInputEOS);
                if (inputConsumed) {
                    inputFrameIndex++;
                    presentationTimeUsCurrent += frameDuration;
                    consumedInputEOS = sawInputEOS;
                }
            }

            // Get output from the encoder
            MediaCodecOutput out = codec.getOutput();
            if (out.outputGenerated) {
                // Detect output EOS
                if ((out.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                    sawOutputEOS = true;
                }

                if (out.buffer.length > 0) {
                    // Save frame
                    if (ivf != null) {
                        ivf.writeFrame(out.buffer, out.outPresentationTimeUs);
                    }

                    // Update statistics - store presentation time delay in offset
                    long presentationTimeUsDelta = out.inPresentationTimeUs -
                            out.outPresentationTimeUs;
                    BufferInfo bufferInfoCopy = new BufferInfo();
                    bufferInfoCopy.set(out.buffer.length,
                            out.outPresentationTimeUs, presentationTimeUsDelta,
                            out.inputRtcTimeUs, out.outputRtcTimeUs, out.flags);
                    bufferInfos.add(bufferInfoCopy);
                }
            }

            // If codec is not ready to accept input/poutput - wait for buffer ready callback
            if ((!inputConsumed || consumedInputEOS) && !out.outputGenerated) {
                codec.waitForBufferEvent();
            }
        }

        codec.deleteCodec();
        if (ivf != null) {
            ivf.close();
        }
        yuvStream.close();

        return bufferInfos;
    }

    /**
     * Vp8 encoding loop supporting encoding multiple streams at a time.
     * Each output stream is described by encodingParams parameters allowing
     * simultaneous encoding of various resolutions, bitrates with an option to
     * control key frame and dynamic bitrate for each output stream indepandently.
     *
     * MediaCodec will raise an IllegalStateException
     * whenever vp8 encoder fails to encode a frame.
     *
     * Color format of input file should be YUV420, and srcFrameWidth,
     * srcFrameHeight should be supplied correctly as raw input file doesn't
     * include any header data.
     *
     * @param srcFrameWidth     Frame width of input yuv file
     * @param srcFrameHeight    Frame height of input yuv file
     * @param encodingParams    Codec stream parameters
     * @return                  Returns 2D array of encoded frames information for each stream and
     *                          for each frame.
     */
    protected ArrayList<ArrayList<BufferInfo>> encodeSimulcast(
            int srcFrameWidth,
            int srcFrameHeight,
            ArrayList<CodecStreamParameters> encodingParams)  throws Exception {
        int numEncoders = encodingParams.size();

        // Create arrays of input/output, formats, bitrates etc
        ArrayList<ArrayList<BufferInfo>> bufferInfos =
                new ArrayList<ArrayList<BufferInfo>>(numEncoders);
        InputStream yuvStream[] = new InputStream[numEncoders];
        IvfWriter[] ivf = new IvfWriter[numEncoders];
        MediaFormat[] format = new MediaFormat[numEncoders];
        MediaCodecAsync[] codec = new MediaCodecAsync[numEncoders];
        int[] inputFrameIndex = new int[numEncoders];
        boolean[] sawInputEOS = new boolean[numEncoders];
        boolean[] consumedInputEOS = new boolean[numEncoders];
        boolean[] inputConsumed = new boolean[numEncoders];
        boolean[] bufferConsumed = new boolean[numEncoders];
        boolean[] sawOutputEOS = new boolean[numEncoders];
        byte[][] srcFrame = new byte[numEncoders][];
        boolean sawOutputEOSTotal = false;
        boolean bufferConsumedTotal = false;
        CodecProperties[] codecProperties = new CodecProperties[numEncoders];
        double frameDuration[] = new double[numEncoders];
        double presentationTimeUsCurrent[] = new double[numEncoders];

        for (int i = 0; i < numEncoders; i++) {
            CodecStreamParameters params = encodingParams.get(i);
            CodecProperties properties = getVp8CodecProperties(true, params.forceSwCodec);

            // Check if scaled image was created
            Pair<Integer, Integer> dstDimension =
                    new Pair<Integer, Integer>(params.frameWidth, params.frameHeight);
            if (!mScaledImages.contains(dstDimension)) {
                // resize image
                cacheScaledImage(params.inputYuvFilename, params.inputResourceId,
                        srcFrameWidth, srcFrameHeight,
                        params.scaledYuvFilename,
                        params.frameWidth, params.frameHeight, properties.colorFormat);
                mScaledImages.add(dstDimension);
            }

            // Create buffer info storage
            bufferInfos.add(new ArrayList<BufferInfo>());

            // Create YUV reader
            yuvStream[i] = new FileInputStream(params.scaledYuvFilename);

            // Create IVF writer
            ivf[i] = new IvfWriter(params.encodedIvfFilename, params.frameWidth, params.frameHeight);

            // Frame buffer
            int frameSize = params.frameWidth * params.frameHeight * 3 / 2;
            srcFrame[i] = new byte[frameSize];

            // Create a media format signifying desired output.
            int bitrate = params.bitrateSet[0];
            int framerate = params.frameRateSet[0];
            format[i] = MediaFormat.createVideoFormat(VP8_MIME,
                    params.frameWidth, params.frameHeight);
            format[i].setInteger(MediaFormat.KEY_BIT_RATE, bitrate);
            if (params.bitrateType == VIDEO_ControlRateConstant) {
                format[i].setInteger("bitrate-mode", VIDEO_ControlRateConstant); // set CBR
            }
            if (params.temporalLayers == 1) {
                format[i].setString("ts-schema", "webrtc.vp8.1-layer");
            } else if (params.temporalLayers == 2) {
                format[i].setString("ts-schema", "webrtc.vp8.2-layer");
            } else if (params.temporalLayers == 3) {
                format[i].setString("ts-schema", "webrtc.vp8.3-layer");
            }
            format[i].setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
            format[i].setInteger(MediaFormat.KEY_FRAME_RATE, framerate);
            int syncFrameInterval = (params.syncFrameInterval + framerate / 2) / framerate; //in sec
            format[i].setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, syncFrameInterval);

            // Input timestamps
            frameDuration[i] = 1000000.0 / framerate;
            presentationTimeUsCurrent[i] = 0;

            // Create encoder
            Log.d(TAG, "Creating encoder #" + i +" : " + properties.codecName +
                    ". Color format: 0x" + Integer.toHexString(properties.colorFormat)+ " : " +
                    params.frameWidth + " x " + params.frameHeight +
                    ". Bitrate: " + bitrate + " Bitrate type: " + params.bitrateType +
                    ". Fps:" + framerate + ". TS Layers: " + params.temporalLayers +
                    ". Key frame:" + syncFrameInterval * framerate +
                    ". Force keyFrame: " + params.syncForceFrameInterval);
            Log.d(TAG, "  Format: " + format[i]);
            Log.d(TAG, "  Input  yuv:" + params.scaledYuvFilename);
            Log.d(TAG, "  Output ivf:" + params.encodedIvfFilename);

            codec[i] = new MediaCodecAsync();
            codec[i].createCodec(i, properties.codecName, format[i],
                    params.timeoutDequeue, true, params.runInLooperThread);
            codecProperties[i] = new CodecProperties(properties.codecName, properties.colorFormat);

            inputConsumed[i] = true;
        }

        while (!sawOutputEOSTotal) {
            // Feed input buffer to all encoders
            for (int i = 0; i < numEncoders; i++) {
                bufferConsumed[i] = false;
                if (consumedInputEOS[i]) {
                    continue;
                }

                CodecStreamParameters params = encodingParams.get(i);
                // Read new input buffers - if previous input was consumed and no EOS
                if (inputConsumed[i] && !sawInputEOS[i]) {
                    int bytesRead = yuvStream[i].read(srcFrame[i]);

                    // Check EOS
                    if (params.frameCount > 0 && inputFrameIndex[i] >= params.frameCount) {
                        sawInputEOS[i] = true;
                    }

                    if (!sawInputEOS[i] && bytesRead == -1) {
                        if (params.frameCount == 0) {
                            sawInputEOS[i] = true;
                        } else {
                            yuvStream[i].close();
                            yuvStream[i] = new FileInputStream(params.scaledYuvFilename);
                            bytesRead = yuvStream[i].read(srcFrame[i]);
                        }
                    }

                    // Convert YUV420 to NV12 if necessary
                    if (codecProperties[i].colorFormat !=
                            CodecCapabilities.COLOR_FormatYUV420Planar) {
                        srcFrame[i] = YUV420ToNV(params.frameWidth, params.frameHeight, srcFrame[i],
                                codecProperties[i].isGoogleSwCodec());
                    }
                }

                inputConsumed[i] = codec[i].feedInput(srcFrame[i],
                        (int)(presentationTimeUsCurrent[i] + 0.5), sawInputEOS[i]);
                if (inputConsumed[i]) {
                    inputFrameIndex[i]++;
                    presentationTimeUsCurrent[i] += frameDuration[i];
                    consumedInputEOS[i] = sawInputEOS[i];
                    bufferConsumed[i] = true;
                }

            }

            // Get output from all encoders
            for (int i = 0; i < numEncoders; i++) {
                if (sawOutputEOS[i]) {
                    continue;
                }

                MediaCodecOutput out = codec[i].getOutput();
                if (out.outputGenerated) {
                    bufferConsumed[i] = true;
                    // Detect output EOS
                    if ((out.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                        sawOutputEOS[i] = true;
                    }

                    if (out.buffer.length > 0) {
                        // Save frame
                        ivf[i].writeFrame(out.buffer, out.outPresentationTimeUs);

                        // Update statistics - store presentation time delay in offset
                        long presentationTimeUsDelta = out.inPresentationTimeUs -
                                out.outPresentationTimeUs;
                        BufferInfo bufferInfoCopy = new BufferInfo();
                        bufferInfoCopy.set(out.buffer.length,
                                out.outPresentationTimeUs, presentationTimeUsDelta,
                                out.inputRtcTimeUs, out.outputRtcTimeUs, out.flags);
                        bufferInfos.get(i).add(bufferInfoCopy);
                    }
                }
            }

            // If codec is not ready to accept input/output - wait for buffer ready callback
            bufferConsumedTotal = false;
            for (boolean bufferConsumedCurrent : bufferConsumed) {
                bufferConsumedTotal |= bufferConsumedCurrent;
            }
            if (!bufferConsumedTotal) {
                // Pick the encoder to wait for
                for (int i = 0; i < numEncoders; i++) {
                    if (!bufferConsumed[i] && !sawOutputEOS[i]) {
                        codec[i].waitForBufferEvent();
                        break;
                    }
                }
            }

            // Check if EOS happened for all encoders
            sawOutputEOSTotal = true;
            for (boolean sawOutputEOSStream : sawOutputEOS) {
                sawOutputEOSTotal &= sawOutputEOSStream;
            }
        }

        for (int i = 0; i < numEncoders; i++) {
            codec[i].deleteCodec();
            ivf[i].close();
            yuvStream[i].close();
        }
        return bufferInfos;
    }


    /**
     * Vp8 encoding and decoding loop supporting encoding and decoding multiple streams
     * at a time. Encoding parameters are described by encodingParams allowing
     * simultaneous encoding of various resolutions and bitrates. Decoding parameters are
     * described by decodingParams.
     *
     * Color format of input file should be YUV420, and srcFrameWidth,
     * srcFrameHeight should be supplied correctly as raw input file doesn't
     * include any header data.
     *
     * @param srcFrameWidth     Frame width of input yuv file
     * @param srcFrameHeight    Frame height of input yuv file
     * @param streamMapping     Arrays of encoder indexes used for decoder input
     * @param encodingParams    Encoding stream parameters
     * @param decodingParams    Decoding stream parameters
     * @return                  Returns 2D array of encoded frames information for each stream and
     *                          for each frame.
     */
    protected ArrayList<ArrayList<BufferInfo>> encodeAndDecode(
            int srcFrameWidth,
            int srcFrameHeight,
            int[] streamMapping,
            ArrayList<CodecStreamParameters> encodingParams,
            ArrayList<CodecStreamParameters> decodingParams)  throws Exception {
        assertTrue(streamMapping.length == decodingParams.size());
        int numEncoders = encodingParams.size();
        int numDecoders = decodingParams.size();

        // Create arrays of input/output, formats, bitrates etc
        ArrayList<ArrayList<BufferInfo>> bufferInfos =
                new ArrayList<ArrayList<BufferInfo>>(numEncoders);
        InputStream yuvInput[] = new InputStream[numEncoders];
        IvfWriter[] ivf = new IvfWriter[numEncoders];
        FileOutputStream[] yuvOutput = new FileOutputStream[numDecoders];

        MediaFormat[] formatEncoder = new MediaFormat[numEncoders];
        MediaFormat[] formatDecoder = new MediaFormat[numDecoders];
        MediaCodecAsync[] encoder = new MediaCodecAsync[numEncoders];
        MediaCodecAsync[] decoder = new MediaCodecAsync[numDecoders];
        MediaCodecOutput[] encoderOutput = new MediaCodecOutput[numEncoders];

        int[] encoderInputFrameIndex = new int[numEncoders];
        boolean[] encoderConsumedInput = new boolean[numEncoders];
        boolean[] encoderSawInputEOS = new boolean[numEncoders];
        boolean[] encoderConsumedInputEOS = new boolean[numEncoders];
        boolean[] encoderCanGenerateOutput = new boolean[numEncoders];
        boolean[] encoderGeneratedOutput = new boolean[numEncoders];
        boolean[] encoderGeneratedOutputEOS = new boolean[numEncoders];

        int[] decoderOutputFrameIndex = new int[numDecoders];
        boolean[] decoderConsumedInput = new boolean[numDecoders];
        boolean[] decoderConsumedInputEOS = new boolean[numDecoders];
        boolean[] decoderGeneratedOutputEOS = new boolean[numDecoders];

        byte[][] inputYuvFrame = new byte[numEncoders][];
        boolean sawOutputEOSTotal = false;
        CodecProperties[] encoderProperties = new CodecProperties[numEncoders];
        double frameDuration[] = new double[numEncoders];
        double presentationTimeUsCurrent[] = new double[numEncoders];
        int[] frameWidth = new int[numDecoders];
        int[] frameHeight = new int[numDecoders];

        // Create encoders
        for (int i = 0; i < numEncoders; i++) {
            CodecStreamParameters params = encodingParams.get(i);
            CodecProperties properties = getVp8CodecProperties(true, params.forceSwCodec);

            // Check if scaled image was created
            Pair<Integer, Integer> dstDimension =
                    new Pair<Integer, Integer>(params.frameWidth, params.frameHeight);
            if (!mScaledImages.contains(dstDimension)) {
                // resize image
                cacheScaledImage(params.inputYuvFilename, params.inputResourceId,
                        srcFrameWidth, srcFrameHeight,
                        params.scaledYuvFilename,
                        params.frameWidth, params.frameHeight, properties.colorFormat);
                mScaledImages.add(dstDimension);
            }

            // Create buffer info storage
            bufferInfos.add(new ArrayList<BufferInfo>());

            // Create YUV reader
            yuvInput[i] = new FileInputStream(params.scaledYuvFilename);

            // Create IVF writer
            ivf[i] = new IvfWriter(params.encodedIvfFilename,
                    params.frameWidth, params.frameHeight);

            // Frame buffer
            int frameSize = params.frameWidth * params.frameHeight * 3 / 2;
            inputYuvFrame[i] = new byte[frameSize];

            // Create a media format signifying desired output.
            int bitrate = params.bitrateSet[0];
            int framerate = params.frameRateSet[0];
            formatEncoder[i] = MediaFormat.createVideoFormat(VP8_MIME,
                    params.frameWidth, params.frameHeight);
            formatEncoder[i].setInteger(MediaFormat.KEY_BIT_RATE, bitrate);
            if (params.bitrateType == VIDEO_ControlRateConstant) {
                formatEncoder[i].setInteger("bitrate-mode", VIDEO_ControlRateConstant); // set CBR
            }
            if (params.temporalLayers == 1) {
                formatEncoder[i].setString("ts-schema", "webrtc.vp8.1-layer");
            } else if (params.temporalLayers == 2) {
                formatEncoder[i].setString("ts-schema", "webrtc.vp8.2-layer");
            } else if (params.temporalLayers == 3) {
                formatEncoder[i].setString("ts-schema", "webrtc.vp8.3-layer");
            }
            formatEncoder[i].setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
            formatEncoder[i].setInteger(MediaFormat.KEY_FRAME_RATE, framerate);
            int syncFrameInterval = (params.syncFrameInterval + framerate / 2) / framerate; //in sec
            formatEncoder[i].setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, syncFrameInterval);

            // Input timestamps
            frameDuration[i] = 1000000.0 / framerate;
            presentationTimeUsCurrent[i] = 0;

            // Create encoder
            Log.d(TAG, "Creating encoder #" + i +" : " + properties.codecName +
                    ". Color format: 0x" + Integer.toHexString(properties.colorFormat)+ " : " +
                    params.frameWidth + " x " + params.frameHeight +
                    ". Bitrate: " + bitrate + " Bitrate type: " + params.bitrateType +
                    ". Fps:" + framerate + ". TS Layers: " + params.temporalLayers +
                    ". Key frame:" + syncFrameInterval * framerate +
                    ". Force keyFrame: " + params.syncForceFrameInterval);
            Log.d(TAG, "  Format: " + formatEncoder[i]);
            Log.d(TAG, "  Input  yuv:" + params.scaledYuvFilename);
            Log.d(TAG, "  Output ivf:" + params.encodedIvfFilename);

            encoder[i] = new MediaCodecAsync();
            encoder[i].createCodec(i, properties.codecName, formatEncoder[i],
                    params.timeoutDequeue, true, false);
            encoderProperties[i] = new CodecProperties(properties.codecName, properties.colorFormat);

            encoderConsumedInput[i] = true;
            encoderCanGenerateOutput[i] = true;
        }

        // Create decoders
        for (int i = 0; i < numDecoders; i++) {
            CodecStreamParameters params = decodingParams.get(i);
            CodecProperties properties = getVp8CodecProperties(false, params.forceSwCodec);

            yuvOutput[i] = null;
            if (params.outputYuvFilename != null) {
                yuvOutput[i] = new FileOutputStream(params.outputYuvFilename, false);
            }

            // Create decoder.
            frameWidth[i] = params.frameWidth;
            frameHeight[i] = params.frameHeight;
            formatDecoder[i] = MediaFormat.createVideoFormat(VP8_MIME, frameWidth[i], frameHeight[i]);
            formatDecoder[i].setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
            Log.d(TAG, "Creating decoder #" + i + ": " + properties.codecName +
                    ". Color format: 0x" + Integer.toHexString(properties.colorFormat) +
                    ". " + frameWidth[i] + " x " + frameHeight[i]);
            Log.d(TAG, "  Format: " + formatDecoder[i]);
            Log.d(TAG, "  Output yuv: " + params.outputYuvFilename);

            decoder[i] = new MediaCodecAsync();
            decoder[i].createCodec(i, properties.codecName, formatDecoder[i],
                    params.timeoutDequeue, false, false);
        }

        // Run encode - decode loop
        while (!sawOutputEOSTotal) {

            // Read input yuv frames from file
            for (int i = 0; i < numEncoders; i++) {
                if (encoderSawInputEOS[i]) {
                    continue;
                }
                if (!encoderConsumedInput[i]) {
                    continue;  // previous input was not cnsumed by encoder yet
                }
                CodecStreamParameters params = encodingParams.get(i);
                int bytesRead = yuvInput[i].read(inputYuvFrame[i]);
                // Check EOS
                if (params.frameCount > 0 && encoderInputFrameIndex[i] >= params.frameCount) {
                    encoderSawInputEOS[i] = true;
                }
                if (!encoderSawInputEOS[i] && bytesRead == -1) {
                    if (params.frameCount == 0) {
                        encoderSawInputEOS[i] = true;
                    } else {
                        yuvInput[i].close();
                        yuvInput[i] = new FileInputStream(params.scaledYuvFilename);
                        bytesRead = yuvInput[i].read(inputYuvFrame[i]);
                    }
                }
                // Convert YUV420 to NV12 if necessary
                if (encoderProperties[i].colorFormat !=
                        CodecCapabilities.COLOR_FormatYUV420Planar) {
                    inputYuvFrame[i] = YUV420ToNV(params.frameWidth, params.frameHeight,
                            inputYuvFrame[i], encoderProperties[i].isGoogleSwCodec());
                }
            }

            // Feed input buffer to all encoders
            for (int i = 0; i < numEncoders; i++) {
                if (encoderConsumedInputEOS[i]) {
                    continue;
                }
                encoderConsumedInput[i] = encoder[i].feedInput(inputYuvFrame[i],
                        (int)(presentationTimeUsCurrent[i] + 0.5), encoderSawInputEOS[i]);
                if (encoderConsumedInput[i]) {
                    encoderInputFrameIndex[i]++;
                    presentationTimeUsCurrent[i] += frameDuration[i];
                    encoderConsumedInputEOS[i] = encoderSawInputEOS[i];
                }
            }

            // Get output from all encoders
            for (int i = 0; i < numEncoders; i++) {
                if (encoderGeneratedOutputEOS[i]) {
                    encoderGeneratedOutput[i] = false;
                    continue;
                }
                // Check if decoders have consumed last encoder output,
                // and if not we can not continue to encode
                if (!encoderCanGenerateOutput[i]) {
                    continue;
                }
                encoderOutput[i] = encoder[i].getOutput();
                encoderGeneratedOutput[i] = encoderOutput[i].outputGenerated;
                if (encoderGeneratedOutput[i]) {
                    // Detect output EOS
                    if ((encoderOutput[i].flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                        encoderGeneratedOutputEOS[i] = true;
                    }
                    if (encoderOutput[i].buffer.length > 0) {
                        // Save ivf output
                        ivf[i].writeFrame(encoderOutput[i].buffer,
                                encoderOutput[i].outPresentationTimeUs);

                        // Update statistics - store presentation time delay in offset
                        long presentationTimeUsDelta = encoderOutput[i].inPresentationTimeUs -
                                encoderOutput[i].outPresentationTimeUs;
                        BufferInfo bufferInfoCopy = new BufferInfo();
                        bufferInfoCopy.set(encoderOutput[i].buffer.length,
                                encoderOutput[i].outPresentationTimeUs, presentationTimeUsDelta,
                                encoderOutput[i].inputRtcTimeUs, encoderOutput[i].outputRtcTimeUs,
                                encoderOutput[i].flags);
                        bufferInfos.get(i).add(bufferInfoCopy);
                    }
                }
            }

            // Feed encoder output to decoder input
            for (int i = 0; i < numEncoders; i++) {
                encoderCanGenerateOutput[i] = true;
            }
            for (int i = 0; i < numDecoders; i++) {
                if (decoderConsumedInputEOS[i]) {
                    continue;
                }
                int j = streamMapping[i];  // encoder index
                if (!encoderGeneratedOutput[j]) {
                    continue;  // no output from encoder available yet
                }
                decoderConsumedInput[i] = decoder[i].feedInput(encoderOutput[j].buffer,
                        (int)(encoderOutput[j].outPresentationTimeUs + 0.5),
                        encoderGeneratedOutputEOS[j]);
                if (decoderConsumedInput[i]) {
                    decoderConsumedInputEOS[i] = encoderGeneratedOutputEOS[j];
                }
                encoderCanGenerateOutput[j] &= decoderConsumedInput[i];
            }

            // Get output from all decoders
            for (int i = 0; i < numDecoders; i++) {
                if (decoderGeneratedOutputEOS[i]) {
                    continue;
                }
                MediaCodecOutput decoderOutput = decoder[i].getOutput();
                if (decoderOutput.outputGenerated) {
                    // Detect output EOS
                    if ((decoderOutput.flags & MediaCodec.BUFFER_FLAG_END_OF_STREAM) != 0) {
                        decoderGeneratedOutputEOS[i] = true;
                    }
                    if (decoderOutput.buffer.length > 0 && yuvOutput[i] != null) {
                        // Save decoder output to yuv file.
                        // Convert NV12 to YUV420 if necessary
                        if (decoderOutput.colorFormat !=
                                CodecCapabilities.COLOR_FormatYUV420Planar) {
                            decoderOutput.buffer = NV12ToYUV420(frameWidth[i], frameHeight[i],
                                    decoderOutput.stride, decoderOutput.sliceHeight,
                                    decoderOutput.buffer);
                        }
                        int writeLength = Math.min(frameWidth[i] * frameHeight[i] * 3 / 2,
                                decoderOutput.buffer.length);
                        // Pack frame if necessary.
                        if (writeLength < decoderOutput.buffer.length &&
                                (decoderOutput.stride > frameWidth[i] ||
                                 decoderOutput.sliceHeight > frameHeight[i])) {
                            decoderOutput.buffer = PackYUV420(frameWidth[i], frameHeight[i],
                                    decoderOutput.stride, decoderOutput.sliceHeight,
                                    decoderOutput.buffer);
                        }
                        yuvOutput[i].write(decoderOutput.buffer, 0, writeLength);
                    }
                    // Check if requested number of frames has been generated, but no
                    // frame with EOS flag was detected to prevent possible dead loop
                    // for codecs with incorrect EOS handling.
                    int j = streamMapping[i];  // encoder index
                    CodecStreamParameters params = encodingParams.get(j);
                    if (params.frameCount > 0 && decoderOutputFrameIndex[i] >= params.frameCount &&
                            !decoderGeneratedOutputEOS[i]) {
                        throw new RuntimeException("No EOS at decoder #" + i +
                                " output at last frame " + params.frameCount);
                    }
                    decoderOutputFrameIndex[i]++;
                }
            }

            // Check if EOS happened for all decoders
            sawOutputEOSTotal = true;
            for (boolean sawOutputEOSStream : decoderGeneratedOutputEOS) {
                sawOutputEOSTotal &= sawOutputEOSStream;
            }
        }

        for (int i = 0; i < numEncoders; i++) {
            encoder[i].deleteCodec();
            ivf[i].close();
            yuvInput[i].close();
        }
        for (int i = 0; i < numDecoders; i++) {
            decoder[i].deleteCodec();
            if (yuvOutput[i] != null) {
                yuvOutput[i].close();
            }
        }

        return bufferInfos;
    }

    /**
     * Delete temporary ivf and yuv output files
     */
    public void deleteTemporaryFiles(CodecStreamParameters params) {
        // Delete ivf
        if (params.encodedIvfFilename != null) {
            File ivfFile = new File(params.encodedIvfFilename);
            ivfFile.delete();
        }
        // Delete output yuv
        if (params.outputYuvFilename != null) {
            File yuvFile = new File(params.outputYuvFilename);
            yuvFile.delete();
        }
    }

    public void deleteTemporaryFiles(ArrayList<CodecStreamParameters> paramsList) {
        for (CodecStreamParameters params : paramsList) {
            deleteTemporaryFiles(params);
        }
    }

    /**
     * Some encoding statistics.
     */
    protected class Vp8EncodingStatistics {
        Vp8EncodingStatistics() {
            mBitrates = new ArrayList<Integer>();
            mFrames = new ArrayList<Integer>();
            mKeyFrames = new ArrayList<Integer>();
            mMinimumKeyFrameInterval = Integer.MAX_VALUE;
        }

        public ArrayList<Integer> mBitrates;// Bitrate values for each second of the encoded stream.
        public ArrayList<Integer> mFrames; // Number of frames in each second of the encoded stream.
        public int mAverageBitrate;         // Average stream bitrate.
        public ArrayList<Integer> mKeyFrames;// Stores the position of key frames in a stream.
        public int mAverageKeyFrameInterval; // Average key frame interval.
        public int mMaximumKeyFrameInterval; // Maximum key frame interval.
        public int mMinimumKeyFrameInterval; // Minimum key frame interval.
    }

    /**
     * Calculates average bitrate and key frame interval for the encoded streams.
     * Output mBitrates field will contain bitrate values for every second
     * of the encoded stream.
     * Average stream bitrate will be stored in mAverageBitrate field.
     * mKeyFrames array will contain the position of key frames in the encoded stream and
     * mKeyFrameInterval - average key frame interval.
     */
    protected Vp8EncodingStatistics computeEncodingStatistics(int encoderId,
            ArrayList<BufferInfo> bufferInfos ) {
        Vp8EncodingStatistics statistics = new Vp8EncodingStatistics();

        int totalSize = 0;
        int frames = 0;
        int framesPerSecond = 0;
        int totalFrameSizePerSecond = 0;
        int maxFrameSize = 0;
        int currentSecond;
        int nextSecond = 0;
        String keyFrameList = "  IFrame List: ";
        String bitrateList = "  Bitrate list: ";
        String framesList = "  FPS list: ";


        for (int j = 0; j < bufferInfos.size(); j++) {
            BufferInfo info = bufferInfos.get(j);
            currentSecond = (int)(info.presentationTimeUs / 1000000);
            boolean lastFrame = (j == bufferInfos.size() - 1);
            if (!lastFrame) {
                nextSecond = (int)(bufferInfos.get(j+1).presentationTimeUs / 1000000);
            }

            totalSize += info.size;
            totalFrameSizePerSecond += info.size;
            maxFrameSize = Math.max(maxFrameSize, info.size);
            framesPerSecond++;
            frames++;

            // Update the bitrate statistics if the next frame will
            // be for the next second
            if (lastFrame || nextSecond > currentSecond) {
                int currentBitrate = totalFrameSizePerSecond * 8;
                bitrateList += (currentBitrate + " ");
                framesList += (framesPerSecond + " ");
                statistics.mBitrates.add(currentBitrate);
                statistics.mFrames.add(framesPerSecond);
                totalFrameSizePerSecond = 0;
                framesPerSecond = 0;
            }

            // Update key frame statistics.
            if ((info.flags & MediaCodec.BUFFER_FLAG_SYNC_FRAME) != 0) {
                statistics.mKeyFrames.add(j);
                keyFrameList += (j + "  ");
            }
        }
        int duration = (int)(bufferInfos.get(bufferInfos.size() - 1).presentationTimeUs / 1000);
        duration = (duration + 500) / 1000;
        statistics.mAverageBitrate = (int)(((long)totalSize * 8) / duration);

        Log.d(TAG, "Statistics for encoder # " + encoderId);
        // Calculate average key frame interval in frames.
        int keyFrames = statistics.mKeyFrames.size();
        if (keyFrames > 1) {
            statistics.mAverageKeyFrameInterval =
                    statistics.mKeyFrames.get(keyFrames - 1) - statistics.mKeyFrames.get(0);
            statistics.mAverageKeyFrameInterval =
                    Math.round((float)statistics.mAverageKeyFrameInterval / (keyFrames - 1));
            for (int j = 1; j < keyFrames; j++) {
                int keyFrameInterval =
                        statistics.mKeyFrames.get(j) - statistics.mKeyFrames.get(j - 1);
                statistics.mMaximumKeyFrameInterval =
                        Math.max(statistics.mMaximumKeyFrameInterval, keyFrameInterval);
                statistics.mMinimumKeyFrameInterval =
                        Math.min(statistics.mMinimumKeyFrameInterval, keyFrameInterval);
            }
            Log.d(TAG, "  Key frame intervals: Max: " + statistics.mMaximumKeyFrameInterval +
                    ". Min: " + statistics.mMinimumKeyFrameInterval +
                    ". Avg: " + statistics.mAverageKeyFrameInterval);
        }
        Log.d(TAG, "  Frames: " + frames + ". Duration: " + duration +
                ". Total size: " + totalSize + ". Key frames: " + keyFrames);
        Log.d(TAG, keyFrameList);
        Log.d(TAG, bitrateList);
        Log.d(TAG, framesList);
        Log.d(TAG, "  Bitrate average: " + statistics.mAverageBitrate);
        Log.d(TAG, "  Maximum frame size: " + maxFrameSize);

        return statistics;
    }

    protected Vp8EncodingStatistics computeEncodingStatistics(
            ArrayList<BufferInfo> bufferInfos ) {
        return computeEncodingStatistics(0, bufferInfos);
    }

    protected ArrayList<Vp8EncodingStatistics> computeSimulcastEncodingStatistics(
            ArrayList<ArrayList<BufferInfo>> bufferInfos) {
        int numCodecs = bufferInfos.size();
        ArrayList<Vp8EncodingStatistics> statistics = new ArrayList<Vp8EncodingStatistics>();

        for (int i = 0; i < numCodecs; i++) {
            Vp8EncodingStatistics currentStatistics =
                    computeEncodingStatistics(i, bufferInfos.get(i));
            statistics.add(currentStatistics);
        }
        return statistics;
    }

    /**
     * Calculates maximum latency for encoder/decoder based on buffer info array
     * generated either by encoder or decoder.
     */
    protected int maxPresentationTimeDifference(ArrayList<BufferInfo> bufferInfos) {
        int maxValue = 0;
        for (BufferInfo bufferInfo : bufferInfos) {
            maxValue = Math.max(maxValue,  (int)bufferInfo.presentationTimeUsDelta);
        }
        maxValue = (maxValue + 500) / 1000; // mcs -> ms
        return maxValue;
    }

    /**
     * Calculates average encoding/decoding time in us based on buffer info array
     * generated either by encoder or decoder.
     */
    protected void averageCodecTimeUs(int id, ArrayList<BufferInfo> bufferInfos) {
        int maximumLatencyUs = 0;
        int averageLatencyUs = 0;
        int averageCodecUs = 0;
        int lastCodecTimeUs = bufferInfos.get(0).outputRtcTimeUs;
        int totalFrames = bufferInfos.size();
        for (BufferInfo bufferInfo : bufferInfos) {
            int currentLatencyUs = bufferInfo.outputRtcTimeUs - bufferInfo.inputRtcTimeUs;
            int currentDecodeUs = bufferInfo.outputRtcTimeUs - lastCodecTimeUs;
            averageLatencyUs += currentLatencyUs;
            averageCodecUs += currentDecodeUs;
            maximumLatencyUs = Math.max(maximumLatencyUs, currentLatencyUs);
            lastCodecTimeUs = bufferInfo.outputRtcTimeUs;
        }
        averageLatencyUs /= totalFrames;
        averageCodecUs /= totalFrames;
        Log.d(TAG, "Codec #" + id + ". Latency avg: " +
                averageLatencyUs + ". max: " + maximumLatencyUs);
        Log.d(TAG, "Processing time: " + lastCodecTimeUs + ". Average: " + averageCodecUs);
        return;
    }

    protected void averageCodecsTimeUs(ArrayList<ArrayList<BufferInfo>> bufferInfos) {
        int numCodecs = bufferInfos.size();
        for (int i = 0; i < numCodecs; i++) {
            averageCodecTimeUs(i, bufferInfos.get(i));
        }
        return;
    }

    /**
     * Decoding PSNR statistics.
     */
    protected class Vp8DecodingStatistics {
        Vp8DecodingStatistics() {
            mMinimumPSNR = Integer.MAX_VALUE;
        }
        public double mAveragePSNR;
        public double mMinimumPSNR;
    }

    /**
     * Calculates PSNR value between two video frames.
     */
    private double computePSNR(byte[] data0, byte[] data1) {
        long squareError = 0;
        assertTrue(data0.length == data1.length);
        int length = data0.length;
        for (int i = 0 ; i < length; i++) {
            int diff = ((int)data0[i] & 0xff) - ((int)data1[i] & 0xff);
            squareError += diff * diff;
        }
        double meanSquareError = (double)squareError / length;
        double psnr = 10 * Math.log10((double)255 * 255 / meanSquareError);
        return psnr;
    }

    /**
     * Calculates average and minimum PSNR values between
     * set of reference and decoded video frames.
     * Runs PSNR calculation for the full duration of the decoded data.
     */
    protected Vp8DecodingStatistics computeDecodingStatisticsEx(
            String referenceYuvFilename,
            int referenceYuvRawId,
            String decodedYuvFilename,
            int width,
            int height,
            int rateDecimator) throws Exception {
        Vp8DecodingStatistics statistics = new Vp8DecodingStatistics();
        InputStream referenceStream =
                OpenFileOrResourceId(referenceYuvFilename, referenceYuvRawId);
        InputStream decodedStream = new FileInputStream(decodedYuvFilename);

        int ySize = width * height;
        int uvSize = width * height / 4;
        byte[] yRef = new byte[ySize];
        byte[] yDec = new byte[ySize];
        byte[] uvRef = new byte[uvSize];
        byte[] uvDec = new byte[uvSize];

        int frames = 0;
        double averageYPSNR = 0;
        double averageUPSNR = 0;
        double averageVPSNR = 0;
        double minimumYPSNR = Integer.MAX_VALUE;
        double minimumUPSNR = Integer.MAX_VALUE;
        double minimumVPSNR = Integer.MAX_VALUE;
        int minimumPSNRFrameIndex = 0;

        while (true) {
            // Calculate Y PSNR.
            int bytesReadDec = decodedStream.read(yDec);
            if (bytesReadDec == -1) {
                break;
            }
            int bytesReadRef = referenceStream.read(yRef);
            if (bytesReadRef == -1) {
                // Reference file wrapping up
                referenceStream.close();
                referenceStream = OpenFileOrResourceId(referenceYuvFilename, referenceYuvRawId);
                bytesReadRef = referenceStream.read(yRef);
            }

            double curYPSNR = computePSNR(yRef, yDec);
            averageYPSNR += curYPSNR;
            minimumYPSNR = Math.min(minimumYPSNR, curYPSNR);
            double curMinimumPSNR = curYPSNR;

            // Calculate U PSNR.
            bytesReadRef = referenceStream.read(uvRef);
            bytesReadDec = decodedStream.read(uvDec);
            double curUPSNR = computePSNR(uvRef, uvDec);
            averageUPSNR += curUPSNR;
            minimumUPSNR = Math.min(minimumUPSNR, curUPSNR);
            curMinimumPSNR = Math.min(curMinimumPSNR, curUPSNR);

            // Calculate V PSNR.
            bytesReadRef = referenceStream.read(uvRef);
            bytesReadDec = decodedStream.read(uvDec);
            double curVPSNR = computePSNR(uvRef, uvDec);
            averageVPSNR += curVPSNR;
            minimumVPSNR = Math.min(minimumVPSNR, curVPSNR);
            curMinimumPSNR = Math.min(curMinimumPSNR, curVPSNR);

            // Frame index for minimum PSNR value - help to detect possible distortions
            if (curMinimumPSNR < statistics.mMinimumPSNR) {
                statistics.mMinimumPSNR = curMinimumPSNR;
                minimumPSNRFrameIndex = frames;
            }

            String logStr = String.format(Locale.US, "PSNR #%d: Y: %.2f. U: %.2f. V: %.2f",
                    frames, curYPSNR, curUPSNR, curVPSNR);
            //Log.v(TAG, logStr);

            frames++;

            // Skip reference frames if rate decimation is enabled
            if (rateDecimator > 1) {
                long bytesToSkip = (rateDecimator - 1) * (ySize + 2 * uvSize);
                while (bytesToSkip > 0) {
                    bytesReadRef = referenceStream.read(yRef);
                    if (bytesReadRef == -1) {
                        referenceStream.close();
                        referenceStream =
                                OpenFileOrResourceId(referenceYuvFilename, referenceYuvRawId);
                        bytesReadRef = referenceStream.read(yRef);
                    }
                    bytesReadRef += referenceStream.read(uvRef);
                    bytesReadRef += referenceStream.read(uvRef);
                    bytesToSkip -= bytesReadRef;
                }
            }
        }

        averageYPSNR /= frames;
        averageUPSNR /= frames;
        averageVPSNR /= frames;
        statistics.mAveragePSNR = (4 * averageYPSNR + averageUPSNR + averageVPSNR) / 6;

        Log.d(TAG, "PSNR statistics for " + frames + " frames.");
        String logStr = String.format(Locale.US,
                "Average PSNR: Y: %.1f. U: %.1f. V: %.1f. Average: %.2f",
                averageYPSNR, averageUPSNR, averageVPSNR, statistics.mAveragePSNR);
        Log.d(TAG, logStr);
        logStr = String.format(Locale.US,
                "Minimum PSNR: Y: %.1f. U: %.1f. V: %.1f. Overall: %.2f at frame %d",
                minimumYPSNR, minimumUPSNR, minimumVPSNR,
                statistics.mMinimumPSNR, minimumPSNRFrameIndex);
        Log.d(TAG, logStr);

        referenceStream.close();
        decodedStream.close();
        return statistics;
    }

    protected Vp8DecodingStatistics computeDecodingStatistics(
            String referenceYuvFilename,
            int referenceYuvRawId,
            String decodedYuvFilename,
            int width,
            int height) throws Exception {
        return computeDecodingStatisticsEx(
                referenceYuvFilename,
                referenceYuvRawId,
                decodedYuvFilename,
                width,
                height,
                1 );
    }
}

